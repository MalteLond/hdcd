% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/binary_segmentation.R
\name{BinarySegmentation}
\alias{BinarySegmentation}
\title{BinarySegmentation}
\usage{
BinarySegmentation(x, delta, lambda, method = c("nodewise_regression",
  "summed_regression", "ratio_regression"), penalize_diagonal = F,
  optimizer = c("line_search", "section_search"), control = NULL,
  standardize = T, threshold = 1e-07, verbose = FALSE, FUN = NULL, ...)
}
\arguments{
\item{x}{A n times p matrix or data frame.}

\item{delta}{Numeric value between 0 and 0.5. This tuning parameter determines
the minimal segment size proportional to the size of the dataset and hence
an upper bound for the number of changepoints (roughly \eqn{1/\delta}).}

\item{lambda}{Positive numeric value. This is the regularization parameter in
the single Lasso fits. This value is ignored if FUN is not NULL.}

\item{method}{Which estimator should be used? Possible choices are \itemize{
\item \strong{nodewise_regression}: Nodewise regression is based on a single
node that needs to be specified with an additional parameter \code{node}
pointing to the column index of the node of interest. Uses
\code{\link[glmnet]{glmnet}} internally. See Kovács (2016) for details.
\item \strong{summed_regression}: Summed nodewise regression sums up the
residual variances of nodewise regression over all nodes. Uses
\code{\link[glasso]{glasso}} internally. See Kovács (2016) for details.
\item \strong{ratio_regression}: Likelihood ratio based regression sums the
pseudo-profile-likelihood over all nodes. Uses \code{\link[glasso]{glasso}}
internally. See Kovács (2016) for details. \item \strong{glasso}: The
graphical Lasso uses the approach of Friedman et al (2007). In contrast to
the other approaches the exact likelihood the whole graphical model is
computed and used as loss. } This value is ignored if \code{FUN} is not
\code{NULL}.}

\item{penalize_diagonal}{Boolean, should the diagonal elements of the
precision matrix be penalized by \eqn{\lambda}? This value is ignored if FUN
is not NULL.}

\item{optimizer}{Which search technique should be used for performing
individual splits in the binary segmentation alogrithm? Possible choices are
\itemize{ \item \strong{line_search}: Exhaustive linear search. All possivle
split candidates are evaluated and the index with maximal loss reduction is
returned. \item \strong{section_search}: Iteratively cuts the search space
according by a flexible ratio as determined by parameter \code{stepsize} in
\code{control} parameter list and approximately finds an index at a local
maximum. See Haubner (2018) for details. }}

\item{control}{A list with parameters that is accessed by the selected
optimizer: \itemize{ \item \strong{stepsize}: Numeric value between 0 and
0.5. Used by section search. \item \strong{min_points}: Integer value larger
than 3. Used by section search.}}

\item{standardize}{Boolean. If TRUE the penalty parameter \eqn{\lambda} will
be adjusted for every dimension in the single Lasso fits according to the
standard deviation in the data.}

\item{threshold}{The threshold for halting the iteration in
\code{\link[glasso]{glasso}} or \code{\link[glmnet]{glmnet}}. In the former
it controls the absolute change of single parameters in the latter it
controls the total objective value. This value is ignored if FUN is not
NULL.}

\item{verbose}{Boolean. If TRUE additional information will be printed.}

\item{FUN}{A loss function with formal arguments, \code{x}, \code{n_obs} and
\code{standardize} which returns a scalar representing the loss for the
segment the function is applied to.}

\item{...}{Supply additional arguments for a specific method (e.g. \code{node}
for \strong{nodewise_regression}) or own loss function \code{FUN}}
}
\value{
An object of class \strong{bs_tree} and \strong{Node} (as defined in
 \code{\link[data.tree]{Node}}).
}
\description{
Applies the binary segmentation algorithmn by recursively calling
\code{\link{FindBestSplit}} in order to build a binary tree. The tree can then
be pruned using \code{\link{PruneTreeGamma}} in order to obtain a changepoint
estimate. Typically this function is not used directly but the interface
\code{\link{hdcd}}.
}
\section{References}{


 Friedman, J., Hastie, T. & Tibshirani, R. Sparse inverse covariance
 estimation with the graphical lasso. Biostatistics 9, 432–441 (2008).

 Haubner, L. Optimistic binary segmentation: A scalable approach to
 changepoint detection in high-dimensional graphical models. (Seminar for
 Statistics, ETH Zurich, 2018).

 Kovács, S. Changepoint detection for high-dimensional covariance matrix
 estimation. (Seminar for Statistics, ETH Zurich, 2016).
}

\examples{
# Use summed regression loss function and ChainNetwork
dat <- SimulateFromModel(CreateModel(n_segments = 2,n = 50,p = 30, ChainNetwork))
res <- BinarySegmentation(dat, delta = 0.1, lambda = 0.01, method = "summed_regression")
print(res)

# Define your own loss function and pass it to the BinarySegmentation algorithm

InitNaiveSquaredLoss <- function(x){

  n_obs <- NROW(x)

  function(x, start, end){

    stopifnot(end >= start && end <= n_obs && start >= 1)

    sum((x - mean(x))^2) / n_obs

  }
}

p <- 5
n <- 20
mean_vecs <- list(rep(0, p), c(rep(1, p-2), rep(5, 2)), rep(-0.5, p))

model <- CreateModel(3, n, p, DiagMatrix, equispaced = TRUE, mean_vecs = mean_vecs)

x <- SimulateFromModel(model)

res <- BinarySegmentation(x, delta = 0.1, lambda = 0.01, FUN = InitNaiveSquaredLoss,
optimizer = "section_search")
print(res)

res <- BinarySegmentation(x, delta = 0.1, lambda = 0.01, FUN = InitNaiveSquaredLoss,
optimizer = "line_search")
print(res)

}
